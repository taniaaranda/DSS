{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secciones = [\"https://www.diariojornada.com.ar/provincia/\",\n",
    "            \"https://www.diariojornada.com.ar/policiales/\",\n",
    "            \"https://www.diariojornada.com.ar/sociedad/\",\n",
    "            \"https://www.diariojornada.com.ar/deportes/\", \n",
    "            \"https://www.diariojornada.com.ar/paismundo/\",\n",
    "            \"https://www.diariojornada.com.ar/economia/\",\n",
    "            \"https://www.diariojornada.com.ar/espectaculos/\",\n",
    "            \"https://www.diariojornada.com.ar/ciencia/\"] \n",
    "\n",
    "data = []\n",
    "\n",
    "for seccion in secciones:\n",
    "    \n",
    "    # Realizamos la petición a la web\n",
    "    req = requests.get(seccion)\n",
    "\n",
    "    # Comprobamos que la petición nos devuelve un Status Code = 200\n",
    "    status_code = req.status_code\n",
    "    if status_code == 200:\n",
    "         # Pasamos el contenido HTML de la web a un objeto BeautifulSoup()\n",
    "        soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "\n",
    "        # Obtenemos todos los enlaces donde están las noticias\n",
    "        enlaces = soup.find_all('a')\n",
    "        patron = re.compile(r'/\\d')\n",
    "        for e in enlaces:\n",
    "            link = e.get('href')\n",
    "            m = patron.search(link)\n",
    "            if m != None:\n",
    "                # Realizamos la petición al enlace de la noticia\n",
    "                req = requests.get(link)\n",
    "                # Comprobamos que la petición nos devuelve un Status Code = 200\n",
    "                status_code = req.status_code\n",
    "                if status_code == 200:\n",
    "                    soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "                    seccion = soup.find('a').get_text()\n",
    "                    titulo = soup.find('span', id = 'ContentPlaceHolder1_lbl_Titulo').get_text()\n",
    "                    elementos_cuerpo = soup.find('div',id = 'cuerpo')\n",
    "                    vistas = elementos_cuerpo.find('span').get_text()\n",
    "                    cuerpo = elementos_cuerpo.find('p').get_text()\n",
    "                    data.append({'seccion':seccion, 'titulo':titulo, 'vistas':vistas,'cuerpo':cuerpo})\n",
    "    else:\n",
    "        print (\"Status Code %d\" % status_code)\n",
    "\n",
    "with open('noticias.json', 'w') as file:\n",
    "    json.dump(data, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## a. Mostrar cuáles son las 5 noticias más relevantes según cantidad de visitas a nivel global\n",
    "\n",
    "with open('noticias.json') as file:\n",
    "    data = json.load (file)\n",
    "    for noticia in data:\n",
    "        removed = noticia['vistas'].replace('.', '')\n",
    "        noticia['vistas'] = removed\n",
    "    \n",
    "    relevantes = sorted(data,key=lambda k: int(k['vistas']),reverse=True)\n",
    "    \n",
    "    for i in range(5):\n",
    "        print(\"seccion \",relevantes[i]['seccion'], \"\\n\",\n",
    "              \"titulo \",relevantes[i]['titulo'],\"\\n\",\n",
    "              \"vistas\",relevantes[i]['vistas'],\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## a. Mostrar cuáles son las 5 noticias más relevantes segun cantidad de visitas por seccion Provincia\n",
    "\n",
    "with open('noticias.json') as file:\n",
    "    data = json.load (file)\n",
    "    provincia = []\n",
    "    for noticia in data:\n",
    "        if noticia['seccion'] == 'PROVINCIA':\n",
    "            removed = noticia['vistas'].replace('.', '')\n",
    "            noticia['vistas'] = removed\n",
    "            provincia.append(noticia)\n",
    "    \n",
    "    relevantes = sorted(provincia ,key=lambda k: int(k['vistas']),reverse=True)\n",
    "    \n",
    "    for i in range(5):\n",
    "        print(\"seccion \",relevantes[i]['seccion'], \"\\n\",\n",
    "              \"titulo \",relevantes[i]['titulo'],\"\\n\",\n",
    "              \"vistas\",relevantes[i]['vistas'],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## a. Mostrar cuáles son las 5 noticias más relevantes segun cantidad de visitas por seccion Policiales\n",
    "\n",
    "with open('noticias.json') as file:\n",
    "    data = json.load (file)\n",
    "    policiales = []\n",
    "    for noticia in data:\n",
    "        if noticia['seccion'] == 'POLICIALES':\n",
    "            removed = noticia['vistas'].replace('.', '')\n",
    "            noticia['vistas'] = removed\n",
    "            policiales.append(noticia)\n",
    "    \n",
    "    relevantes = sorted(policiales ,key=lambda k: int(k['vistas']),reverse=True)\n",
    "    \n",
    "    for i in range(5):\n",
    "        print(\"seccion \",relevantes[i]['seccion'], \"\\n\",\n",
    "              \"titulo \",relevantes[i]['titulo'],\"\\n\",\n",
    "              \"vistas\",relevantes[i]['vistas'],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## a. Mostrar cuáles son las 5 noticias más relevantes segun cantidad de visitas por seccion Sociedad\n",
    "\n",
    "with open('noticias.json') as file:\n",
    "    data = json.load (file)\n",
    "    sociedad = []\n",
    "    for noticia in data:\n",
    "        if noticia['seccion'] == 'SOCIEDAD':\n",
    "            removed = noticia['vistas'].replace('.', '')\n",
    "            noticia['vistas'] = removed\n",
    "            sociedad.append(noticia)\n",
    "    \n",
    "    relevantes = sorted(sociedad ,key=lambda k: int(k['vistas']),reverse=True)\n",
    "    \n",
    "    for i in range(5):\n",
    "        print(\"seccion \",relevantes[i]['seccion'], \"\\n\",\n",
    "              \"titulo \",relevantes[i]['titulo'],\"\\n\",\n",
    "              \"vistas\",relevantes[i]['vistas'],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## a. Mostrar cuáles son las 5 noticias más relevantes segun cantidad de visitas por seccion Deportes\n",
    "\n",
    "with open('noticias.json') as file:\n",
    "    data = json.load (file)\n",
    "    deportes = []\n",
    "    for noticia in data:\n",
    "        if noticia['seccion'] == 'DEPORTES':\n",
    "            removed = noticia['vistas'].replace('.', '')\n",
    "            noticia['vistas'] = removed\n",
    "            deportes.append(noticia)\n",
    "    \n",
    "    relevantes = sorted(deportes ,key=lambda k: int(k['vistas']),reverse=True)\n",
    "    \n",
    "    for i in range(5):\n",
    "        print(\"seccion \",relevantes[i]['seccion'], \"\\n\",\n",
    "              \"titulo \",relevantes[i]['titulo'],\"\\n\",\n",
    "              \"vistas\",relevantes[i]['vistas'],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## a. Mostrar cuáles son las 5 noticias más relevantes segun cantidad de visitas por seccion Pais y Mundo\n",
    "\n",
    "with open('noticias.json') as file:\n",
    "    data = json.load (file)\n",
    "    pais_mundo = []\n",
    "    for noticia in data:\n",
    "        if noticia['seccion'] == 'PA\\u00cdS & MUNDO':\n",
    "            removed = noticia['vistas'].replace('.', '')\n",
    "            noticia['vistas'] = removed\n",
    "            pais_mundo.append(noticia)\n",
    "    \n",
    "    relevantes = sorted(pais_mundo ,key=lambda k: int(k['vistas']),reverse=True)\n",
    "    \n",
    "    for i in range(5):\n",
    "        print(\"seccion \",relevantes[i]['seccion'], \"\\n\",\n",
    "              \"titulo \",relevantes[i]['titulo'],\"\\n\",\n",
    "              \"vistas\",relevantes[i]['vistas'],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## a. Mostrar cuáles son las 5 noticias más relevantes segun cantidad de visitas por seccion Economia\n",
    "\n",
    "with open('noticias.json') as file:\n",
    "    data = json.load (file)\n",
    "    economia = []\n",
    "    for noticia in data:\n",
    "        if noticia['seccion'] == 'ECONOM\\u00cdA':\n",
    "            removed = noticia['vistas'].replace('.', '')\n",
    "            noticia['vistas'] = removed\n",
    "            economia.append(noticia)\n",
    "    \n",
    "    relevantes = sorted(economia ,key=lambda k: int(k['vistas']),reverse=True)\n",
    "    \n",
    "    for i in range(5):\n",
    "        print(\"seccion \",relevantes[i]['seccion'], \"\\n\",\n",
    "              \"titulo \",relevantes[i]['titulo'],\"\\n\",\n",
    "              \"vistas\",relevantes[i]['vistas'],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## a. Mostrar cuáles son las 5 noticias más relevantes segun cantidad de visitas por seccion Espectaculos\n",
    "\n",
    "with open('noticias.json') as file:\n",
    "    data = json.load (file)\n",
    "    espectaculos = []\n",
    "    for noticia in data:\n",
    "        if noticia['seccion'] == 'ESPECT\\u00c1CULOS':\n",
    "            removed = noticia['vistas'].replace('.', '')\n",
    "            noticia['vistas'] = removed\n",
    "            espectaculos.append(noticia)\n",
    "    \n",
    "    relevantes = sorted(espectaculos ,key=lambda k: int(k['vistas']),reverse=True)\n",
    "    \n",
    "    for i in range(5):\n",
    "        print(\"seccion \",relevantes[i]['seccion'], \"\\n\",\n",
    "              \"titulo \",relevantes[i]['titulo'],\"\\n\",\n",
    "              \"vistas\",relevantes[i]['vistas'],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## a. Mostrar cuáles son las 5 noticias más relevantes segun cantidad de visitas por seccion Ciencia\n",
    "\n",
    "with open('noticias.json') as file:\n",
    "    data = json.load (file)\n",
    "    ciencia = []\n",
    "    for noticia in data:\n",
    "        if noticia['seccion'] == 'CIENCIA':\n",
    "            removed = noticia['vistas'].replace('.', '')\n",
    "            noticia['vistas'] = removed\n",
    "            ciencia.append(noticia)\n",
    "    \n",
    "    relevantes = sorted(ciencia ,key=lambda k: int(k['vistas']),reverse=True)\n",
    "    \n",
    "    for i in range(5):\n",
    "        print(\"seccion \",relevantes[i]['seccion'], \"\\n\",\n",
    "              \"titulo \",relevantes[i]['titulo'],\"\\n\",\n",
    "              \"vistas\",relevantes[i]['vistas'],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## b) Mostrar cómo están distribuidas las palabras de cierto artı́culo\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "palabras_omitir = ['ya','el', 'la','lo', 'los', 'las', 'un', 'una', 'unos', 'unas','a', 'ante',\n",
    "                  'bajo','cabe','como','con','contra','de','desde','durante','en','entre','excepto',\n",
    "                  'hacia','hace','hasta','mediante','para','por','segun','sin','sobre','tras',\n",
    "                  'del','que','se','muchos','muchas','mucho','mucha','hace','y','o','e','Mucho',\n",
    "                  'han','ha','he','Y','al','ello','conlleva','todos','todo','todas','toda','es',\n",
    "                   'sea','Muchas','algunas','algunos','pero','no','si','pocas','“Una','da','-y','más','siguen',\n",
    "                   'parte','esta','Pero','otra','otro','–casi','ninguna-','siendo','Este','también',\n",
    "                  'sino','sido','sigue','siendo-','aún','mayor','habría','Las','vez','menos','gran',\n",
    "                  'cuales','ser','Por','ni','En','últimas','basado','Se','como:-','su','además','sólo',\n",
    "                  'sus','misma','antes','esta.','deben','misma']\n",
    "with open('noticias.json') as file:\n",
    "    data = json.load (file)\n",
    "    lista_palabras_sin_omitir = data[0]['cuerpo'].split()\n",
    "    lista_palabras = []\n",
    "    for w in lista_palabras_sin_omitir:\n",
    "        if w not in palabras_omitir:\n",
    "            lista_palabras.append(w)\n",
    "\n",
    "    frecuencia_palabras = {}\n",
    "    frecuencia = []\n",
    "    for w in lista_palabras:\n",
    "        if w not in frecuencia_palabras:\n",
    "            cant = lista_palabras.count(w)\n",
    "            frecuencia_palabras[w] = cant\n",
    "            frecuencia.append(cant)\n",
    "    sns.set()\n",
    "    sns.distplot(frecuencia,color=\"violet\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMEDIO DE PALABRAS  467.6923076923077\n",
      "MÁXIMO DE PALABRAS  990\n",
      "MÍNIMO DE PALABRAS  147\n",
      "PROMEDIO DE SENTENCIAS  16.692307692307693\n",
      "MÁXIMO DE SENTENCIAS  36\n",
      "MÍNIMO DE SENTENCIAS  4\n",
      "PROMEDIO DE PÁRRAFOS  15.692307692307692\n",
      "MÁXIMO DE PÁRRAFOS  35\n",
      "MÍNIMO DE PÁRRAFOS  3\n"
     ]
    }
   ],
   "source": [
    "## c) Mostrar el número promedio, máximo y mı́nimo de palabras, sentencias, párrafos de un conjunto de noticias de su preferencia. \n",
    "import numpy as np\n",
    "import statistics as stats\n",
    "\n",
    "with open('noticias.json') as file:\n",
    "    data = json.load (file)\n",
    "    palabras = []\n",
    "    sentencias = []\n",
    "    parrafos = []\n",
    "    for noticia in data:\n",
    "        if noticia['seccion'] == 'CIENCIA':\n",
    "            palabras.append(len(noticia['cuerpo'].split()))\n",
    "            sentencias.append(noticia['cuerpo'].count('.'))\n",
    "            parrafos.append(len(re.findall(r'[.].',noticia['cuerpo'])))\n",
    "\n",
    "    print(\"PROMEDIO DE PALABRAS \", stats.mean(palabras))\n",
    "    print(\"MÁXIMO DE PALABRAS \", max(palabras))\n",
    "    print(\"MÍNIMO DE PALABRAS \", min(palabras))\n",
    "    \n",
    "    print(\"PROMEDIO DE SENTENCIAS \", stats.mean(sentencias))\n",
    "    print(\"MÁXIMO DE SENTENCIAS \", max(sentencias))\n",
    "    print(\"MÍNIMO DE SENTENCIAS \", min(sentencias))\n",
    "    \n",
    "    print(\"PROMEDIO DE PÁRRAFOS \", stats.mean(parrafos))\n",
    "    print(\"MÁXIMO DE PÁRRAFOS \", max(parrafos))\n",
    "    print(\"MÍNIMO DE PÁRRAFOS \", min(parrafos))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
